{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPokY6/caGrvhe6pthrKY9R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#create simple dataset manually\n","data_set={\n","    'sqt_living': [650,800,1000,1200,1400,1600,1800,2000,2200,2400],\n","    'price': [100000,120000,140000,160000,180000,200000,220000,240000,260000,300000]\n","}\n","\n","#convert to dataframe\n","home_data=pd.DataFrame(data_set)\n","\n","print('sample dataset created successfully')\n","print(home_data)\n","\n","#prepare feature(x) and target(y)\n","X=home_data[['sqt_living']].values #features must be 2D\n","y=home_data['price'].values #Target(1D)\n","\n","# splittin the dataset into training and test set\n","\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=1/3,random_state=0)\n","from sklearn.linear_model import LinearRegression\n","Jitendratyagi = LinearRegression()\n","Jitendratyagi.fit(X_train, y_train)\n","\n","#prediction of test and training set results\n","y_pred=Jitendratyagi.predict(X_test)\n","y_pred_train=Jitendratyagi.predict(X_train)\n","\n","#print model parameters and simple metrics\n","print(\"\\nmodel slope(coefficient):\",Jitendratyagi.coef_)\n","print(\"model intercept:\",Jitendratyagi.intercept_)\n","\n","from sklearn.metrics import mean_squared_error,r2_score\n","print(\"\\nTrain R^2:\",r2_score(y_train,y_pred_train))\n","print(\"Test R^2:\",r2_score(y_test,y_pred))\n","print(\"Train RMSE:\",np.sqrt(mean_squared_error(y_train,y_pred_train)))\n","\n","#visualize the training results with a smooth regression line\n","\n","plt.figure(figsize=(8,5))\n","\n","# create smooth line for regression\n","line_x = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n","line_y = Jitendratyagi.predict(line_x)\n","plt.plot(line_x, line_y, label='Regression line', color='green', linewidth=2)\n","\n","# visualize training data points\n","plt.scatter(X_train, y_train, label='Training data', color='blue')\n","\n","plt.title('House price vs living area(Training set)')\n","plt.xlabel('Living area (sqft) ')\n","plt.ylabel('House price ($)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","x=load_iris().data\n","y=load_iris().target\n","\n","load_iris().target\n","\n","x_train, x_test , y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n","model = LogisticRegression()\n","model.fit(x_train,y_train)\n","predictions = model.predict(x_test)\n","print(\"Accuracy:\", accuracy_score(y_test, predictions))\n","print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n","\n","# Visualize the decision boundaries\n","# Using only the first two features for simplicity in visualization\n","X_vis = x[:, :2]\n","y_vis = y\n","\n","x_min, x_max = X_vis[:, 0].min() - .5, X_vis[:, 0].max() + .5\n","y_min, y_max = X_vis[:, 1].min() - .5, X_vis[:, 1].max() + .5\n","h = .02  # step size in the mesh\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","# Train a logistic regression model on the first two features for visualization\n","model_vis = LogisticRegression()\n","model_vis.fit(X_vis, y_vis)\n","\n","Z = model_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","plt.figure(1, figsize=(8, 6))\n","plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","# Plot also the training points\n","plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolors='k', cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","plt.title('Logistic Regression Decision Boundaries (using Sepal Length and Sepal Width)')\n","\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xticks(())\n","plt.yticks(())\n","\n","plt.show()\n","\n","import pandas as pd\n","\n","#create your own dataset\n","data={\n","    'study_hours':[1,2,3,4,5,6,7,8,9,10],\n","    'attendance(%)':[55,60,65,70,75,80,85,90,95,100],\n","    'internal_marks':[60,65,70,75,80,85,90,95,100,105],\n","    'result':['pass','pass','pass','pass','pass','pass','pass','pass','fail','fail']\n","}\n","\n","#convert to dataframe\n","df = pd.DataFrame(data)\n","\n","#show dataset\n","print(\"student performance dataset\")\n","print(df)\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier , plot_tree\n","from sklearn.metrics import accuracy_score , classification_report , confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","#prepare data\n","x=df[['study_hours','attendance(%)','internal_marks']]\n","y=df['result']\n","\n","#split data\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","#train model\n","model=DecisionTreeClassifier(criterion='entropy',random_state=0)\n","model.fit(x_train,y_train)\n","\n","#predict\n","y_pred=model.predict(x_test)\n","\n","#evaluate\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualize\n","plt.figure(figsize=(10,8))\n","plot_tree(model,feature_names=x.columns,class_names=['Fail','pass'],filled=True)\n","plt.show()\n","\n","#random forests\n","#libraries\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","#load the dataset\n","iris=load_iris()\n","print('df')\n","display(df)\n","\n","#create dataframe for better visulaization\n","df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n","df['species']=iris.target\n","print('df')\n","display(df)\n","\n","#create dataframe for better visulaization\n","df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n","df['species']=iris.target\n","\n","#show dataset\n","print(\"iris dataset\")\n","print(df.head())\n","\n","#dataset info\n","print(\"\\ndataset info\")\n","print(df.info())\n","print(df.describe())\n","print(df.head())\n","\n","#split data into feature and labels\n","x=df.iloc[:,:-1]\n","y=df.iloc[:,-1]\n","\n","#split data (80%,20%)\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","#model training\n","model=RandomForestClassifier(n_estimators=100,random_state=0)\n","model.fit(x_train,y_train)\n","\n","# predict\n","y_pred=model.predict(x_test)\n","\n","#evaluate\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualize\n","plt.figure(figsize=(20,10))\n","plot_tree(model.estimators_[0],\n","          feature_names=x.columns,\n","          class_names=iris.target_names,\n","          filled=True,\n","          rounded=True,\n","          fontsize=10)\n","plt.title('Visualization of the First Decision Tree in the Random Forest')\n","plt.show()\n","\n","#import required libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#load the iris dataset\n","iris = load_iris()\n","\n","#convert dataset to dataframe\n","df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n","df['species'] = iris.target\n","\n","#display first  5 rows\n","print(df.head())\n","\n","# split into feature(x) and target(y)\n","x = df.iloc[:, :-1]\n","y = df.iloc[:, -1]\n","\n","#split into training and testing set\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","#feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc=StandardScaler()\n","x_train=sc.fit_transform(x_train)\n","x_test=sc.transform(x_test)\n","\n","#create and train knn classifier\n","knn=KNeighborsClassifier(n_neighbors=3)\n","knn.fit(x_train,y_train)\n","\n","#make predictions\n","y_pred=knn.predict(x_test)\n","\n","#evaluate\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualise\n","plt.figure(figsize=(8,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('K-Nearest Neighbors Classification')\n","plt.show()\n","\n","#Bagging\n","\n","#ibraries implementation\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#load dataset\n","iris=load_iris()\n","x,y=iris.data,iris.target\n","\n","#split into train/test\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","#create base model\n","base_model=DecisionTreeClassifier()\n","\n","#create bagging model\n","bagging_model=BaggingClassifier(estimator=base_model,n_estimators=10,random_state=42)\n","\n","#train both models\n","base_model.fit(x_train,y_train)\n","bagging_model.fit(x_train,y_train)\n","\n","#compare both performance\n","base_pred=base_model.predict(x_test)\n","bagging_pred=bagging_model.predict(x_test)\n","\n","# evaluation\n","print(\"Base Model Accuracy:\",accuracy_score(y_test,base_pred))\n","print(\"Bagging Model Accuracy:\",accuracy_score(y_test,bagging_pred))\n","\n","#visualization\n","plt.figure(figsize=(10,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=bagging_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='Bagging')\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('Bagging Classification')\n","plt.legend()\n","plt.show()\n","\n","#stacking\n","\n","#Libraries required\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#define base learners\n","base_learners=[\n","    ('lr',LogisticRegression()),\n","    ('dt',DecisionTreeClassifier())\n","]\n","\n","#meta learner\n","meta_learner=LogisticRegression()\n","\n","#create stacking model\n","stack_model=StackingClassifier(estimators=base_learners,final_estimator=meta_learner)\n","\n","#train\n","stack_model.fit(x_train,y_train)\n","\n","#evaluate\n","y_pred=stack_model.predict(x_test)\n","\n","#visualize\n","plt.figure(figsize=(10,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='Stacking')\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('Stacking Classification')\n","plt.legend()\n","plt.show()\n","\n","# BOOSTING (ada,gradient,XGBOOST) : Adaboost\n","\n","#libraries required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#create base model\n","base_model=DecisionTreeClassifier(max_depth=1)\n","\n","#create boosting model\n","boosting_model=AdaBoostClassifier(estimator=base_model,n_estimators=50,random_state=42)\n","\n","#train\n","boosting_model.fit(x_train,y_train)\n","\n","# Load and split the dataset\n","iris = load_iris()\n","x, y = iris.data, iris.target\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","#evaluate\n","y_pred=boosting_model.predict(x_test)\n","\n","#evaluation\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualization of adaboost\n","plt.figure(figsize=(10,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='AdaBoost')\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('AdaBoost Classification')\n","plt.legend()\n","plt.show()\n","\n","# BOOSTING (ada,gradient,XGBOOST) : Gradientboost\n","\n","#libraries required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#create base model\n","base_model=DecisionTreeClassifier\n","\n","#create boosting model\n","boosting_model=GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1,random_state=42)\n","\n","#splitting\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","#train\n","boosting_model.fit(x_train,y_train)\n","\n","#predicting\n","y_pred=boosting_model.predict(x_test)\n","\n","#evaluation\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualization\n","plt.figure(figsize=(10,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='X',label='GradientBoost')\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('GradientBoost Classification')\n","plt.legend()\n","plt.show()\n","\n","# BOOSTING (ada,gradient,XGBOOST) : XGboost\n","\n","#libraries required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#create base model(weak learner)\n","base_model=DecisionTreeClassifier(max_depth=1)\n","\n","#create boosting model\n","boosting_model=XGBClassifier(n_estimators=100,learning_rate=1.0,max_depth=1,random_state=42)\n","\n","#train\n","boosting_model.fit(x_train,y_train)\n","\n","#predict\n","y_pred=boosting_model.predict(x_test)\n","\n","#evaluation\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n","print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n","\n","#visualization\n","plt.figure(figsize=(10,6))\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='X',label='XGboost')\n","plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('XGboost Classification')\n","plt.legend()\n","plt.show()\n","\n","#SVM\n","\n","#libraries required\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","#load dataset\n","iris=load_iris()\n","x,y=iris.data,iris.target\n","\n","#split data\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","#train svm model\n","svm_model=SVC(kernel='linear',C=1.0)\n","svm_model.fit(x_train,y_train)\n","\n","#predict and check accuracy\n","y_pred=svm_model.predict(x_test)\n","print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n","\n","#visualize decision boundaries\n","# For visualization purposes, train a new SVM model using only the first two features\n","svm_model_vis = SVC(kernel='linear', C=1.0)\n","svm_model_vis.fit(x_train[:, :2], y_train)\n","\n","x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n","y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n","\n","# Predict on the meshgrid using the model trained on two features\n","z = svm_model_vis.predict(np.array([xx.ravel(), yy.ravel()]).T)\n","z = z.reshape(xx.shape)\n","\n","plt.contourf(xx, yy, z, alpha=0.4, cmap='viridis')\n","plt.scatter(x[:, 0], x[:, 1], c=y, cmap='viridis', edgecolors='k', s=50, marker='o')\n","plt.xlabel('Sepal Length (cm)')\n","plt.ylabel('Sepal Width (cm)')\n","plt.title('SVM Classification (using Sepal Length and Sepal Width)')\n","plt.show()\n","\n","#tensorflow and keras\n","#libraries required\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","#create simple training data\n","x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]],dtype=float)\n","y=np.array([0,0,0,1,1,0,1],dtype=float)\n","\n","#normalize data (important for neural network)\n","x=x / np.amax(x,axis=0)\n","\n","#build the neural network model\n","model=keras.Sequential()\n","model.add(layers.Dense(10,input_dim=2,activation='relu'))\n","model.add(layers.Dense(1,activation='sigmoid'))\n","\n","#compile the model\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","#train the model\n","model.fit(x,y,epochs=200,verbose=0)\n","\n","#test the model with new output\n","test_data=np.array([[4,7]])\n","prediction=model.predict(test_data)\n","\n","print(\"predicted output (1=pass,0=fail):\",prediction)\n","if prediction > 0.5:\n","    print(\"Student will pass\")\n","else:\n","    print(\"Student will fail\")\n","\n","#PCA\n","\n","#libraries required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","#load the dataset\n","iris=load_iris()\n","x,y=iris.data,iris.target\n","\n","#apply pca(reduce 4d->2d)\n","pca=PCA(n_components=2)\n","x_pca=pca.fit_transform(x)\n","\n","#visualize the result\n","plt.scatter(x_pca[:,0],x_pca[:,1],c=y,cmap='viridis',edgecolors='k',s=50,marker='o')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA Visualization')\n","plt.show()\n","\n","#K-means clustering\n","\n","#Libraries required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import KMeans\n","\n","#Create some sample data\n","x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]])\n","#Build k-means model\n","kmeans=KMeans(n_clusters=2,random_state=0)\n","kmeans.fit(x)\n","\n","#Get cluster centers and labels\n","centroids = kmeans.cluster_centers_\n","labels=kmeans.labels_\n","\n","#visualize the clusters\n","plt.scatter(x[:,0],x[:,1],c=labels,cmap='viridis',edgecolors='k',s=50,marker='o')\n","plt.scatter(centroids[:,0],centroids[:,1],c='red',marker='X',s=200,label='Centroids')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('K-means Clustering')\n","plt.show()\n","\n","#hierarchical clustering\n","#importing libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import AgglomerativeClustering\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","#create sample data\n","x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]])\n","\n","# perform hierarchial clustering\n","Z = linkage(X,method='ward')\n","\n","#plot the dendrogram\n","plt.figure(figsize=(10,5))\n","dendrogram(Z,leaf_rotation=90,leaf_font_size=12)\n","plt.xlabel('Data Points')\n","plt.ylabel('Euclidean Distance')\n","plt.title('Dendrogram')\n","plt.show()\n","\n","#form clusters"],"metadata":{"id":"NbjjPPYIZTom"},"execution_count":null,"outputs":[]}]}